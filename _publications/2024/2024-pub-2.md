---
title:          "Mol2Lang-VLM: Vision- and Text-Guided Generative Pre-trained Language Models for Advancing Molecule Captioning through Multimodal Fusion"
date:           2024-08-15 00:00:00 +0700
selected:       true
pub:            "Proceedings of the 1st Workshop on Language + Molecules (ACL 2024 Workshop)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
pub_last:       ' <span class="badge badge-pill badge-publication badge-pub-conf">Conf</span>'
pub_date:       "2024"

abstract: >-
  This paper introduces Mol2Lang-VLM, an enhanced method for refining generative pre-trained language models for molecule captioning using multimodal features to achieve more accurate caption generation. Our approach leverages the encoder and decoder blocks of the Transformer-based architecture by introducing third sub-layers into both. Specifically, we insert sub-layers in the encoder to fuse features from SELFIES strings and molecular images, while the decoder fuses features from SMILES strings and their corresponding descriptions. Moreover, cross multi-head attention is employed instead of common multi-head attention to enable the decoder to attend to the encoder’s output, thereby integrating the encoded contextual information for better and more accurate caption generation. Performance evaluation on the CheBI-20 and L+M-24 benchmark datasets demonstrates Mol2Lang-VLM’s superiority, achieving higher accuracy and quality in caption generation compared to existing methods. Our code and pre-processed data are available at https://github.com/nhattruongpham/mol-lang-bridge/tree/mol2lang/.
cover:          /assets/images/covers/Mol2Lang-VLM.webp
authors:
  - Duong Thanh Tran*
  - truongpn*
  - nguyenndh
  - bala#
links:
  Paper: https://doi.org/10.18653/v1/2024.langmol-1.12
  Code: https://github.com/nhattruongpham/mol-lang-bridge/tree/mol2lang
---
